{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "from __future__ import print_function #print from python3\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readImages():\n",
    "    #dossier ou se trouvent les images :\n",
    "    folder = \"/home/data/collection/GUIMUTEIC/CLICIDE/CLICIDEMAX/\"\n",
    "    \n",
    "    #on parcourt le dossier :\n",
    "    listImages = glob.glob(folder+'*.JPG')\n",
    "    listImagesTest = glob.glob(folder+'test/*.JPG')\n",
    "    \n",
    "    #extraction de la classe des images\n",
    "    listImagesClass = []\n",
    "    for im in listImages:\n",
    "        c = im.split(\"/\")[-1]\n",
    "        c = c.split(\"-\")[0]\n",
    "        listImagesClass.append(c)\n",
    "    #print(len(listImages) == len(listImagesClass))\n",
    "    listImagesTestClass = []\n",
    "    for im in listImagesTest:\n",
    "        c = im.split(\"/\")[-1]\n",
    "        c = c.split(\"-\")[0]\n",
    "        listImagesTestClass.append(c)\n",
    "    \n",
    "    return (listImages, listImagesClass), (listImagesTest, listImagesTestClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = readImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.426, 0.426, 0.418) (0.2, 0.176, 0.17)\n"
     ]
    }
   ],
   "source": [
    "mean = (0.426, 0.426, 0.418)\n",
    "std_dev = (0.200, 0.176, 0.170)\n",
    "print(mean, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU (inplace)\n",
      "    (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU (inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU (inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU (inplace)\n",
      "    (12): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Linear (9216 -> 4096)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): Dropout (p = 0.5)\n",
      "    (4): Linear (4096 -> 4096)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): Linear (4096 -> 1000)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model1 = models.alexnet(pretrained=True).eval()\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "inputs = torch.Tensor(2,3,224,224)\n",
    "imageOpen = Image.open(train[0][0])\n",
    "imageOpen = imageOpen.resize((224,224), Image.BILINEAR)\n",
    "\n",
    "imageTransformation = transforms.Compose( \n",
    "                        (transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean, std_dev) )\n",
    "                        )\n",
    "\n",
    "inputs[0] = imageTransformation(imageOpen)\n",
    "inputs[1] = imageTransformation(Image.open(train[0][10]).resize((224,224), Image.BILINEAR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "outputs = model1(Variable(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  2.9713  -1.2717   0.9710  ...   -0.9601   0.0638   1.4522\n",
      "  6.7962   0.5667   0.5651  ...    2.1066   0.7941  -3.1658\n",
      "[torch.FloatTensor of size 2x1000]\n",
      "\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(outputs)\n",
    "print(len(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet (\n",
       "  (features): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU (inplace)\n",
       "    (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU (inplace)\n",
       "    (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU (inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU (inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU (inplace)\n",
       "    (12): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Dropout (p = 0.5)\n",
       "    (1): Linear (9216 -> 4096)\n",
       "    (2): ReLU (inplace)\n",
       "    (3): Dropout (p = 0.5)\n",
       "    (4): Linear (4096 -> 4096)\n",
       "    (5): ReLU (inplace)\n",
       "    (6): Linear (4096 -> 1000)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class maxnet(nn.Module):\n",
    "    def __init__(self, nbClass=464):\n",
    "        super(maxnet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d((3, 3), stride=(2, 2), dilation=(1, 1)),\n",
    "                nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d((3, 3), stride=(2, 2), dilation=(1, 1)),\n",
    "                nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d((3, 3), stride=(2, 2), dilation=(1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, nbClass),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "mod = maxnet().train()\n",
    "\n",
    "criterion = nn.loss.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mod.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "imageTransformation = transforms.Compose( \n",
    "                                (transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std_dev) )\n",
    "                                )\n",
    "\n",
    "listLabel = set()\n",
    "for el in train[1]:\n",
    "    if not 'wall' in el:\n",
    "        listLabel.add(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464\n",
      "['10A', '10B', '10C', '10D', '10E', '10F', '10G', '10H', '10I', '10J', '11A', '11B', '11C', '11D', '11E', '11F', '11G', '11H', '11I', '11J', '11K', '11L', '11M', '11N', '11O', '12A', '12B', '12C', '12D', '12E', '12F', '12G', '12H', '12I', '12J', '12K', '12O', '12P', '13A', '13C', '13D', '13E', '13G', '13H', '14B', '14C', '14D', '14E', '14F', '14G', '14H', '14I', '14J', '14K', '14L', '14M', '15A', '15B', '15C', '15D', '15E', '15F', '15G', '15J', '15K', '15L', '15N', '15O', '15P', '15Q', '15R', '15S', '15T', '16A', '16B', '16C', '16D', '16E', '16F', '16G', '16H', '16I', '16J', '16K', '16L', '16M', '16N', '16O', '16P', '16Q', '17A', '17B', '17C', '17D', '17E', '17F', '17G', '17H', '17I', '17J', '17K', '17L', '17M', '17N', '1A', '1B', '1C', '1D', '1E', '1F', '1G', '1H', '1I', '1J', '1K', '1L', '1M', '1N', '1O', '1P', '1Q', '1R', '20A', '20B', '20C', '20D', '20E', '20F', '20G', '20H', '20I', '20J', '20K', '20L', '20M', '20N', '21A', '21B', '21C', '21D', '21E', '21F', '21G', '21H', '21J', '21K', '21L', '21M', '21N', '21O', '22A', '22B', '22C', '22D', '22E', '22F', '22G', '22H', '22I', '22J', '23A', '23B', '23C', '23D', '23E', '23F', '23G', '23H', '23J', '23K', '23L', '23M', '23N', '23O', '24B', '24C', '24D', '24E', '24F', '24G', '24H', '24I', '24J', '24K', '24L', '24P', '24R', '25B', '26A', '26B', '26C', '26D', '26E', '26F', '26G', '26H', '26I', '26J', '26K', '26L', '26M', '26N', '26O', '26P', '26Q', '27A', '27B', '27C', '27D', '27E', '27F', '27G', '27H', '27I', '27J', '27K', '27L', '28A', '28B', '28C', '28D', '28E', '28F', '28G', '28H', '28I', '28J', '28K', '28L', '28M', '28N', '29A', '29B', '29C', '29D', '29E', '29F', '29G', '29H', '29I', '29J', '29K', '2A', '2B', '2C', '2D', '2E', '2F', '2G', '2H', '2I', '2J', '2K', '30A', '30C', '30D', '30E', '30F', '30G', '30H', '30I', '30J', '30L', '30M', '30O', '30P', '30Q', '30R', '31A', '31B', '31C', '32A', '32B', '32C', '32D', '32F', '32G', '32H', '32J', '32K', '32L', '32M', '32N', '32O', '33A', '33B', '33C', '33D', '33E', '33F', '33H', '33I', '33J', '33K', '33L', '33M', '34B', '34C', '34D', '34E', '34G', '34H', '34I', '34J', '34K', '34L', '35A', '35B', '35C', '35D', '35E', '35F', '35G', '35H', '35I', '35J', '35K', '36A', '36B', '36C', '36D', '36E', '36F', '36G', '36H', '36I', '36J', '36K', '36L', '36M', '36N', '36O', '36R', '37A', '37B', '37D', '37E', '37F', '37G', '37I', '37J', '37K', '37L', '38A', '38B', '38C', '38D', '38E', '39A', '39B', '39C', '39E', '39F', '39G', '3A', '3B', '3C', '3D', '3E', '3F', '3G', '3H', '3I', '3J', '3K', '3L', '3M', '3N', '3O', '3P', '3Q', '3R', '3S', '3T', '40A', '40B', '40C', '40E', '40F', '40G', '41A', '41B', '41C', '41E', '42B', '42D', '42E', '42F', '43A', '43C', '43D', '44A', '44C', '44D', '44E', '4A', '4B', '4C', '4D', '4E', '4F', '4G', '4H', '4I', '4K', '4L', '4M', '4N', '4O', '4P', '4Q', '4R', '4S', '4T', '5A', '5B', '5C', '5D', '5E', '5F', '5G', '5H', '5I', '5J', '5J2', '5K', '5L', '5M', '5N', '5O', '5P', '5Q', '6A', '6B', '6C', '6D', '6E', '6F', '6G', '6H', '7A', '7B', '7C', '7D', '7E', '8A', '8B', '8C', '8D', '8E', '8F', '8G', '8H', '8H2', '8I', '8J', '8K', '8L', '8M', '9A', '9B', '9C', '9D', '9E']\n"
     ]
    }
   ],
   "source": [
    "print(len(listLabel))\n",
    "listLabel = sorted(list(listLabel))\n",
    "print(listLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-56039e3aa772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlistLabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(mod.parameters(), lr=0.1, momentum=0.9)\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    #chaque epoch parcourt la collection\n",
    "    for i, im in enumerate(train[0]):\n",
    "        inputs = torch.Tensor(1,3,224,224)\n",
    "        imageOpen = Image.open(im).resize((224,224), Image.BILINEAR)\n",
    "        inputs[0] = imageTransformation(imageOpen)\n",
    "        inputs = Variable(inputs)\n",
    "        \n",
    "        outputs = mod(inputs)\n",
    "        \n",
    "        lab = Variable(torch.LongTensor([listLabel.index(train[1][0])]))\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, lab)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        if i % 10 == 9: # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 10))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
