{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from dataset import ReadImages\n",
    "from utils import *\n",
    "from model.nn_utils import *\n",
    "from model.siamese import *\n",
    "from model.custom_modules import *\n",
    "from os import path\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainSetPath = 'data/pre_proc/CLICIDE_video_384'\n",
    "testSetPath = 'data/pre_proc/CLICIDE_video_384/test'\n",
    "mean_std_file = 'data/CLICIDE_224sq_train_ms.txt'\n",
    "num_classes = 464\n",
    "cnn_model = models.resnet152\n",
    "scale_size = 320\n",
    "feature_size = (7, 7)\n",
    "out_size = 2048 # 1000 # 7 * 7 * 2048  # 6 * 6 * 256\n",
    "siam2_k = 6\n",
    "weights_file = 'data/20170429-150806-986029_best_siam.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3245, 177, 464)\n",
      "['29J', '29K', '29H', '29I', '29F', '29G', '29D', '29E', '29B', '29C', '29A', '34D', '34E', '34G', '34B', '34C', '34L', '34H', '34I', '34J', '34K', '6H', '3S', '3R', '3Q', '3P', '43C', '3T', '3K', '3J', '3I', '3H', '3O', '3N', '3M', '3L', '3C', '3B', '3A', '3G', '3F', '3E', '3D', '10J', '10H', '10I', '10B', '10C', '10A', '10F', '10G', '10D', '10E', '44C', '44A', '44E', '44D', '27A', '27B', '27C', '27D', '27E', '27F', '27G', '27H', '27I', '27J', '27K', '27L', '9A', '9C', '9B', '9E', '9D', '8J', '8K', '8H', '5J2', '8L', '33M', '33L', '33I', '33H', '33K', '33J', '33E', '33D', '33F', '33A', '33C', '33B', '8H2', '14F', '14G', '14D', '14E', '14B', '14C', '14L', '14M', '14J', '14K', '14H', '14I', '23L', '23M', '23N', '23O', '23H', '23J', '23K', '23D', '23E', '23F', '23G', '23A', '23B', '23C', '37I', '37K', '37J', '37L', '37A', '37B', '37E', '37D', '37G', '37F', '40G', '40F', '40E', '40C', '40B', '40A', '2D', '2E', '2F', '2G', '2A', '2B', '2C', '2H', '2I', '2J', '2K', '13C', '13A', '13G', '13E', '13D', '13H', '24E', '24D', '24G', '24F', '24C', '24B', '24L', '24I', '24H', '24K', '24J', '24P', '24R', '39C', '39B', '39A', '8I', '39G', '39F', '39E', '8M', '8B', '8C', '8A', '8F', '8G', '8D', '8E', '1Q', '1P', '1R', '1A', '1C', '1B', '1E', '1D', '1G', '1F', '1I', '1H', '1K', '1J', '1M', '1L', '1O', '1N', '43D', '43A', '17N', '17M', '17L', '17K', '17J', '17I', '17H', '17G', '17F', '17E', '17D', '17C', '17B', '17A', '7E', '7D', '7C', '7B', '7A', '20A', '20C', '20B', '20E', '20D', '20G', '20F', '20I', '20H', '20K', '20J', '20M', '20L', '20N', '32B', '32C', '32A', '32F', '32G', '32D', '32J', '32K', '32H', '32N', '32O', '32L', '32M', '41E', '41A', '41B', '41C', '12A', '12B', '12C', '12D', '12E', '12F', '12G', '12H', '12I', '12J', '12K', '12O', '12P', '38A', '38B', '38C', '38D', '38E', '36N', '36O', '36L', '36M', '36J', '36K', '36H', '36I', '36F', '36G', '36D', '36E', '36B', '36C', '36A', '36R', '31C', '31B', '31A', '16L', '16M', '16N', '16O', '16H', '16I', '16J', '16K', '16D', '16E', '16F', '16G', '16A', '16B', '16C', '16P', '16Q', '6A', '6B', '6C', '6D', '6E', '6F', '6G', '25B', '35G', '35F', '35E', '35D', '35C', '35B', '35A', '35K', '35J', '35I', '35H', '5Q', '5P', '5M', '5L', '5O', '5N', '5I', '5H', '5K', '5J', '5E', '5D', '5G', '5F', '5A', '5C', '5B', '26C', '26B', '26A', '26G', '26F', '26E', '26D', '26K', '26J', '26I', '26H', '26O', '26N', '26M', '26L', '26Q', '26P', '28I', '28H', '28K', '28J', '28M', '28L', '28N', '28A', '28C', '28B', '28E', '28D', '28G', '28F', '21F', '21G', '21D', '21E', '21B', '21C', '21A', '21N', '21O', '21L', '21M', '21J', '21K', '21H', '30P', '30Q', '30R', '30H', '30I', '30J', '30L', '30M', '30O', '30A', '30C', '30D', '30E', '30F', '30G', '42B', '42E', '42D', '42F', '11E', '11D', '11G', '11F', '11A', '11C', '11B', '11M', '11L', '11O', '11N', '11I', '11H', '11K', '11J', '22J', '22I', '22H', '22G', '22F', '22E', '22D', '22C', '22B', '22A', '4T', '4R', '4S', '4P', '4Q', '4N', '4O', '4L', '4M', '4K', '4H', '4I', '4F', '4G', '4D', '4E', '4B', '4C', '4A', '15Q', '15P', '15S', '15R', '15T', '15A', '15C', '15B', '15E', '15D', '15G', '15F', '15K', '15J', '15L', '15O', '15N']\n"
     ]
    }
   ],
   "source": [
    "trainSetFull = ReadImages.readImageswithPattern(\n",
    "    trainSetPath, lambda im: im.split('/')[-1].split('-')[0])\n",
    "testSetFull = ReadImages.readImageswithPattern(\n",
    "    testSetPath, lambda im: im.split('/')[-1].split('-')[0])\n",
    "\n",
    "listLabel = [t[1] for t in trainSetFull if 'wall' not in t[1]]\n",
    "labels = list(set(listLabel))\n",
    "print(len(trainSetFull), len(testSetFull), len(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3245\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "m, s = readMeanStd(mean_std_file)\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize(m, s)])\n",
    "\n",
    "trainSet, testSet, trainNames, testNames = [], [], [], []\n",
    "for im, lab in trainSetFull:\n",
    "    if lab in labels:\n",
    "        im_out = trans(imread_rgb(im))\n",
    "        trainSet.append((im_out, lab))\n",
    "        trainNames.append(im)\n",
    "for im, lab in testSetFull:\n",
    "    if lab in labels:\n",
    "        im_out = trans(imread_rgb(im))\n",
    "        testSet.append((im_out, lab))\n",
    "        testNames.append(im)\n",
    "print(len(trainSet))\n",
    "print(len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tuning_model = TuneClassif(cnn_model(), num_classes, feature_size)\n",
    "tuning_model.load_state_dict(torch.load('data/finetune_classif/cli_best_resnet152_classif_finetuned.pth.tar'))\n",
    "classif_net = TuneClassifSub(tuning_model, num_classes, feature_size)\n",
    "feature_net = FeatureNet(classif_net, feature_size, classify=True)\n",
    "siam_net = Siamese2(feature_net, siam2_k, out_size, feature_size)\n",
    "siam_net.load_state_dict(torch.load(weights_file))\n",
    "siam_net = siam_net.eval().cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(net, dataset, out_size):\n",
    "    def batch(last, i, is_final, batch):\n",
    "        embeddings = last\n",
    "        test_in = batch[0][0].unsqueeze(0).cuda(0)\n",
    "        out = net(Variable(test_in, volatile=True))\n",
    "        embeddings[i] = out.data[0]\n",
    "        return embeddings\n",
    "    init = torch.Tensor(len(dataset), out_size)\n",
    "    return fold_batches(batch, init, dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([165, 3245])\n"
     ]
    }
   ],
   "source": [
    "embeddings_test = get_embeddings(siam_net, testSet, out_size)\n",
    "embeddings_train = get_embeddings(siam_net, trainSet, out_size)\n",
    "sim = torch.mm(embeddings_test, embeddings_train.t())\n",
    "print(sim.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 156/165\n",
      "Incorrect 1: test im data/pre_proc/CLICIDE_video_384/test/11C-0351.JPG, label 11C -> train im data/pre_proc/CLICIDE_video_384/11F-7.JPG, label 11F\n",
      "Avg prec: 0.00229784975425\n",
      "Incorrect 2: test im data/pre_proc/CLICIDE_video_384/test/11C-0436.JPG, label 11C -> train im data/pre_proc/CLICIDE_video_384/10J-1.JPG, label 10J\n",
      "Avg prec: 0.00822340006581\n",
      "Incorrect 3: test im data/pre_proc/CLICIDE_video_384/test/26J-1245.JPG, label 26J -> train im data/pre_proc/CLICIDE_video_384/43D-0.JPG, label 43D\n",
      "Avg prec: 0.000796974303149\n",
      "Incorrect 4: test im data/pre_proc/CLICIDE_video_384/test/26J-1247.JPG, label 26J -> train im data/pre_proc/CLICIDE_video_384/26B-2.JPG, label 26B\n",
      "Avg prec: 0.220404411765\n",
      "Incorrect 5: test im data/pre_proc/CLICIDE_video_384/test/30A-1257.JPG, label 30A -> train im data/pre_proc/CLICIDE_video_384/30J-6.JPG, label 30J\n",
      "Avg prec: 0.100261003283\n",
      "Incorrect 6: test im data/pre_proc/CLICIDE_video_384/test/36E-1265.JPG, label 36E -> train im data/pre_proc/CLICIDE_video_384/3S-0.JPG, label 3S\n",
      "Avg prec: 0.231109469429\n",
      "Incorrect 7: test im data/pre_proc/CLICIDE_video_384/test/4L-0504.JPG, label 4L -> train im data/pre_proc/CLICIDE_video_384/15P-5.JPG, label 15P\n",
      "Avg prec: 0.00367647058824\n",
      "Incorrect 8: test im data/pre_proc/CLICIDE_video_384/test/5B-0506.JPG, label 5B -> train im data/pre_proc/CLICIDE_video_384/14B-0.JPG, label 14B\n",
      "Avg prec: 0.265245623941\n",
      "Incorrect 9: test im data/pre_proc/CLICIDE_video_384/test/8C-0388.JPG, label 8C -> train im data/pre_proc/CLICIDE_video_384/8B-9.JPG, label 8B\n",
      "Avg prec: 0.164541304063\n"
     ]
    }
   ],
   "source": [
    "max_sim, max_idx = sim.max(1)\n",
    "max_label = []\n",
    "for i in range(sim.size(0)):\n",
    "    # get label from ref set which obtained highest score\n",
    "    max_label.append(trainSet[max_idx[i, 0]][1])\n",
    "correct = sum(testLabel == max_label[j] for j, (_, testLabel) in enumerate(testSet))\n",
    "print('Correct: {0}/{1}'.format(correct, len(testSet)))\n",
    "count = 0\n",
    "for j, (_, test_label) in enumerate(testSet):\n",
    "    if max_label[j] == test_label:\n",
    "        continue\n",
    "    count += 1\n",
    "    print('Incorrect {0}: test im {1}, label {2} -> train im {3}, label {4}'.format(count, testNames[j], test_label, trainNames[max_idx[j, 0]], max_label[j]))\n",
    "    print('Avg prec: {0}'.format(avg_precision(sim, j, testSet, trainSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
