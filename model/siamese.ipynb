{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Siamese1(nn.Module):\n",
    "    \"\"\"\n",
    "        Define a siamese network\n",
    "        Given a module, it will duplicate it with weight sharing, concatenate the output and add a linear classifier \n",
    "    \"\"\"\n",
    "    def __init__(self, net):\n",
    "        super(siamese, self).__init__()\n",
    "        self.features = net\n",
    "        self.classifier = nn.Linear(net.classifier[len(net.classifier._modules)-1].out_features*2, 1)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat( (self.features(x1), self.features(x2)), 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Siamese2(nn.Module):\n",
    "    \"\"\"\n",
    "        Define a siamese network\n",
    "        Given a module, it will duplicate it with weight sharing, concatenate the output and add a linear classifier \n",
    "    \"\"\"\n",
    "    def __init__(self, net):\n",
    "        super(Siamese2, self).__init__()\n",
    "        self.features = net\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        return (self.features(x1), self.features(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def siamese():\n",
    "    return Siamese2(models.alexnet(pretrained=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    t = Variable(torch.Tensor(1,3,225,225))\n",
    "    s = Siamese2(models.alexnet(pretrained=True))\n",
    "    o = s(t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# autograd function to normalize an input over the rows\n",
    "# (each vector of a batch is normalized)\n",
    "# the backward step follows the implementation of\n",
    "# torch.legacy.nn.Normalize closely\n",
    "class Normalize2DL2(Function):\n",
    "\n",
    "    def __init__(self, eps=1e-10):\n",
    "        super(Normalize2DL2, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.norm2 = input.pow(2).sum(1).add_(self.eps)\n",
    "        self.norm = self.norm2.pow(0.5)\n",
    "        output = input / self.norm.expand_as(input)\n",
    "        self.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input = self.saved_tensors[0]\n",
    "        gradInput = self.norm2.expand_as(input) * grad_output\n",
    "        cross = (input * grad_output).sum(1)\n",
    "        buf = input * cross.expand_as(input)\n",
    "        gradInput.add_(-1, buf)\n",
    "        cross = self.norm2 * self.norm\n",
    "        gradInput.div_(cross.expand_as(gradInput))\n",
    "        return gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NormalizeL2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NormalizeL2, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return Normalize2DL2()(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_layers(net):\n",
    "    if isinstance(net, models.ResNet):\n",
    "        features = [net.conv1, net.bn1, net.relu, net.maxpool]\n",
    "        features.extend(net.layer1)\n",
    "        features.extend(net.layer2)\n",
    "        features.extend(net.layer3)\n",
    "        features.extend(net.layer4)\n",
    "        features = nn.Sequential(*features)\n",
    "        feature_reduc = nn.Sequential(net.avgpool)\n",
    "        classifier = nn.Sequential(net.fc)\n",
    "    else:\n",
    "        features, classifier = net.features, net.classifier\n",
    "        feature_reduc = nn.Sequential()\n",
    "    return features, feature_reduc, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TuneClassif(nn.Module):\n",
    "    \"\"\"\n",
    "        Image classification network based on a pretrained network\n",
    "        which is then finetuned to a different dataset\n",
    "        It's assumed that the last layer of the given network\n",
    "        is a fully connected (linear) one\n",
    "        untrained_blocks specifies how many layers or blocks of layers are\n",
    "        left untrained (only layers with parameters are counted). for ResNet, each 'BottleNeck' or 'BasicBlock' (block containing skip connection for residual) is considered as one block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net, num_classes, untrained_blocks=-1):\n",
    "        super(TuneClassif, self).__init__()\n",
    "        features, feature_reduc, classifier = extract_layers(net)\n",
    "        if untrained_blocks < 0:\n",
    "            untrained_blocks = sum(1 for _ in features) + sum(1 for _ in classifier)\n",
    "        self.features = features\n",
    "        self.feature_reduc = feature_reduc\n",
    "        self.classifier = classifier\n",
    "        # make sure we never retrain the first few layers\n",
    "        # this is usually not needed\n",
    "        seqs = [self.features, self.feature_reduc, self.classifier]\n",
    "\n",
    "        def has_param(m):\n",
    "            return sum(1 for _ in m.parameters()) > 0\n",
    "        count = 0\n",
    "        for module in (m for seq in seqs for m in seq if has_param(m)):\n",
    "            if count >= untrained_blocks:\n",
    "                break\n",
    "            count += 1\n",
    "            for p in module.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        for name, module in self.classifier._modules.items():\n",
    "            if module is classifier[len(classifier._modules) - 1]:\n",
    "                self.classifier._modules[name] = nn.Linear(module.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.feature_reduc(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Siamese1(nn.Module):\n",
    "    \"\"\"\n",
    "        Define a siamese network\n",
    "        Given a module, it will duplicate it with weight sharing, concatenate the output and add a linear classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, net, num_classes=100, feature_dim=100, feature_size2d=(6, 6)):\n",
    "        super(Siamese1, self).__init__()\n",
    "        self.features = net.features\n",
    "        spatial_factor = 4\n",
    "        self.spatial_feature_reduc = nn.Sequential(\n",
    "            nn.AvgPool2d(spatial_factor)\n",
    "        )\n",
    "        factor = feature_size2d[0] / spatial_factor * feature_size2d[1] / spatial_factor\n",
    "        for module in self.features:\n",
    "            if isinstance(module, models.resnet.Bottleneck):\n",
    "                in_features = module.conv3.out_channels * factor\n",
    "            if isinstance(module, models.resnet.BasicBlock):\n",
    "                in_features = module.conv2.out_channels * factor\n",
    "            if isinstance(module, nn.modules.Conv2d):\n",
    "                in_features = module.out_channels * factor\n",
    "        if feature_dim <= 0:\n",
    "            for module in net.classifier:\n",
    "                if isinstance(module, nn.modules.linear.Linear):\n",
    "                    out_features = module.out_features\n",
    "        else:\n",
    "            out_features = feature_dim\n",
    "        self.feature_reduc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            NormalizeL2(),\n",
    "            nn.Linear(in_features, out_features)\n",
    "        )\n",
    "        self.feature_reduc2 = NormalizeL2()\n",
    "\n",
    "    def forward_single(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.spatial_feature_reduc(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.feature_reduc1(x)\n",
    "        x = self.feature_reduc2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        if self.training:\n",
    "            return self.forward_single(x1), self.forward_single(x2), self.forward_single(x3)\n",
    "        else:\n",
    "            return self.forward_single(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# metric loss according to Chopra et al \"Learning a Similarity Metric Discriminatively, with Application to Face Verification\"\n",
    "# since we assume normalized vectors, we use Q=2\n",
    "class MetricL(Function):\n",
    "\n",
    "    def __init__(self, size_average=True):\n",
    "        super(MetricL, self).__init__()\n",
    "        self.size_average = size_average\n",
    "\n",
    "    # TODO: everything could be done inplace,\n",
    "    # more difficult though (for norm see torch.nn._functions.loss.Cosine...)\n",
    "    def terms(self, input1, input2, y):\n",
    "        diff = input1 - input2\n",
    "        energy = diff.norm(1, 1)\n",
    "        e = energy * 0 + np.e\n",
    "        exp_term = torch.pow(e, -2.77 * energy / 2)\n",
    "        return diff, energy, exp_term\n",
    "\n",
    "    # target takes values in 1 (good), -1 (bad) so (1-target)/2 is 0 for good pairs and 1 for bad ones, (1+target) / 2 inverse\n",
    "    def forward(self, input1, input2, y):\n",
    "        _, energy, exp_term = self.terms(input1, input2, y)\n",
    "        loss_g = (1 + y) * energy * energy / 2\n",
    "        loss_i = (1 - y) * 2 * exp_term\n",
    "        loss = (loss_g + loss_i).sum(0).view(1)\n",
    "        if self.size_average:\n",
    "            loss.div_(y.size(0))\n",
    "        self.save_for_backward(input1, input2, y)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input1, input2, y = self.saved_tensors\n",
    "        diff, energy, exp_term = self.terms(input1, input2, y)\n",
    "        diff[diff.lt(0)] = -1\n",
    "        diff[diff.ge(0)] = 1\n",
    "        y_g = (1 + y).view(-1, 1).expand_as(input1)\n",
    "        y_i = (1 - y).view(-1, 1).expand_as(input1)\n",
    "        energy = energy.expand_as(input1)\n",
    "        exp_term = exp_term.expand_as(input1)\n",
    "        grad1 = y_g * diff * energy - 2.77 * y_i * diff * exp_term\n",
    "        grad2 = -grad1\n",
    "        if self.size_average:\n",
    "            grad1.div_(y.size(0))\n",
    "            grad2.div_(y.size(0))\n",
    "        if grad_output[0] != 1:\n",
    "            grad1.mul_(grad_output)\n",
    "            grad2.mul_(grad_output)\n",
    "        return grad1, grad2, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetricLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, size_average=True):\n",
    "        super(MetricLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input1, input2, target):\n",
    "        return MetricL(self.size_average)(input1, input2, target)\n",
    "\n",
    "\n",
    "class TripletL(Function):\n",
    "\n",
    "    def __init__(self, margin, size_average=True):\n",
    "        super(TripletL, self).__init__()\n",
    "        self.size_average = size_average\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, pos, neg):\n",
    "        sqdiff = anchor.add(-1, pos).pow_(2)\n",
    "        sqdiff = anchor.add(-1, neg).pow_(2)\n",
    "        loss = sqdiff.sum(1)\n",
    "        loss.add_(-1, sqdiff.sum(1))\n",
    "        loss.add_(self.margin)\n",
    "        self.clamp = torch.lt(loss, 0)\n",
    "        loss[self.clamp] = 0\n",
    "        loss = loss.sum(0).view(1)\n",
    "        if self.size_average:\n",
    "            loss.div_(anchor.size(0))\n",
    "        self.save_for_backward(anchor, pos, neg)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # grad_pos = -2(x_anchor - x_pos)\n",
    "        # grad_neg = 2(x_anchor - x_neg)\n",
    "        # grad_anchor = 2(x_anchor - x_pos) - 2(x_anchor - x_neg)\n",
    "        # = -(grad_pos + grad_neg)\n",
    "        anchor, pos, neg = self.saved_tensors\n",
    "        c = self.clamp.expand_as(anchor)\n",
    "        anchor[c] = 0\n",
    "        pos[c] = 0\n",
    "        neg[c] = 0\n",
    "        anchor_sum = anchor.sum(0)\n",
    "        grad_pos = anchor_sum.add(-1, pos.sum(0)).mul_(-2)\n",
    "        grad_neg = anchor_sum.add_(-1, neg.sum(0)).mul_(2)\n",
    "        grad_anchor = grad_pos.add(grad_neg).mul_(-1)\n",
    "\n",
    "        if self.size_average:\n",
    "            grad_anchor.div_(anchor.size(0))\n",
    "            grad_pos.div_(anchor.size(0))\n",
    "            grad_neg.div_(anchor.size(0))\n",
    "        if grad_output[0] != 1:\n",
    "            grad_anchor = grad_anchor.mul_(grad_output)\n",
    "            grad_pos = grad_pos.mul_(grad_output)\n",
    "            grad_neg = grad_neg.mul_(grad_output)\n",
    "        grad_anchor = grad_anchor.expand_as(anchor)\n",
    "        grad_pos = grad_pos.expand_as(anchor)\n",
    "        grad_neg = grad_neg.expand_as(anchor)\n",
    "        return grad_anchor, grad_pos, grad_neg\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, margin, size_average=True):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, pos, neg):\n",
    "        return TripletL(self.margin, self.size_average)(anchor, pos, neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
