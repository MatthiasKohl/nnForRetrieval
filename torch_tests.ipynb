{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "inputs = torch.randn(2, 3, 224, 224)\n",
    "vin1 = Variable(inputs, requires_grad=True)\n",
    "vin2 = Variable(inputs[0].unsqueeze(0), requires_grad=True)\n",
    "vin3 = Variable(inputs[1].unsqueeze(0), requires_grad=True)\n",
    "labels = torch.LongTensor([1,0])\n",
    "lab1 = Variable(labels)\n",
    "lab2 = Variable(torch.LongTensor([1]))\n",
    "lab3 = Variable(torch.LongTensor([0]))\n",
    "\n",
    "net = models.resnet152(pretrained=True).eval()\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=False)\n",
    "out1 = net(vin1)\n",
    "l1 = loss(out1, lab1)\n",
    "l1.backward()\n",
    "\n",
    "v1g = vin1.grad.clone()\n",
    "v1fcg = net.fc.weight.grad.clone()\n",
    "v1conv1g = net.conv1.weight.grad.clone()\n",
    "\n",
    "for p in net.parameters():\n",
    "    void = p.grad.data.zero_()\n",
    "\n",
    "out2 = net(vin2)\n",
    "l2 = loss(out2, lab2)\n",
    "l2.backward()\n",
    "out3 = net(vin3)\n",
    "l3 = loss(out3, lab3)\n",
    "l3.backward()\n",
    "\n",
    "v2g = vin2.grad.clone()\n",
    "v2fcg = net.fc.weight.grad.clone()\n",
    "v2conv1g = net.conv1.weight.grad.clone()\n",
    "v3g = vin3.grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 19.4826\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 9.1400\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 10.3426\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(l1)\n",
    "print(l2)\n",
    "print(l3)\n",
    "\n",
    "def cmp(v1, v2):\n",
    "    return (torch.abs(v1 - v2) > 1e-3).long().sum().data[0]\n",
    "print(cmp(out1[0], out2[0]))\n",
    "print(cmp(out1[1], out3[0]))\n",
    "print(cmp(v1g[1], v3g[0]))\n",
    "print(cmp(v1fcg, v2fcg))\n",
    "print(cmp(v1conv1g, v2conv1g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tests when first playing around with region descriptor net (region pooling in particular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import time\n",
    "from utils import fold_batches\n",
    "from model.custom_modules import TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_classes = 1000\n",
    "epochs = 1\n",
    "mini_batch_size = 64\n",
    "micro_batch_size = 1\n",
    "alpha = 1.0\n",
    "image_size = (3, 300, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = [(torch.randn(*image_size), random.randrange(num_classes)) for _ in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class WeirdNet(nn.Module):\n",
    "    def __init__(self, net, k):\n",
    "        super(WeirdNet, self).__init__()\n",
    "        self.net = net\n",
    "        self.k = k\n",
    "        self.conv_final = nn.Conv2d(2048, 1000, (1, 1))\n",
    "    \n",
    "    def forward_single(self, x):\n",
    "        x = self.net.conv1(x)\n",
    "        x = self.net.bn1(x)\n",
    "        x = self.net.relu(x)\n",
    "        x = self.net.maxpool(x)\n",
    "        x = self.net.layer1(x)\n",
    "        x = self.net.layer2(x)\n",
    "        x = self.net.layer3(x)\n",
    "        \n",
    "        feat = self.net.layer4(x)\n",
    "        x = self.net.avgpool(feat)\n",
    "        x = self.conv_final(x)\n",
    "        \n",
    "        c_maxv, _ = x.max(1)\n",
    "        c_maxv = c_maxv.view(-1)\n",
    "        k = min(c_maxv.size(0), self.k)\n",
    "        _, flat_idx = c_maxv.topk(k)\n",
    "\n",
    "        def feature_idx(flat_idx):\n",
    "            cls_idx = flat_idx // x.size(3), flat_idx % x.size(3)\n",
    "            return (cls_idx[0], cls_idx[0] + 7,\n",
    "                    cls_idx[1], cls_idx[1] + 7)\n",
    "        top_idx = [feature_idx(int(i)) for i in flat_idx.data]\n",
    "        # needed for output\n",
    "        tmp = c_maxv.data.clone().resize_(x.size(0), 2048)\n",
    "        acc = Variable(tmp.fill_(0))\n",
    "        tmp = c_maxv.data.clone().resize_(x.size(0), x.size(1), self.k)\n",
    "        cls_out = Variable(tmp.fill_(0))\n",
    "\n",
    "        i = 0\n",
    "        for x1, x2, y1, y2 in top_idx:\n",
    "            cls_out[:, :, i] = x[:, :, x1, y1]\n",
    "            i += 1\n",
    "            region = feat[:, :, x1, y1]\n",
    "            acc = acc + region\n",
    "        return acc, cls_out\n",
    "\n",
    "#         k = min(self.k, x.size(2) * x.size(3))\n",
    "#         tmp = Variable(feat.data.clone().resize_(feat.size(0), feat.size(1)).fill_(2))\n",
    "#         cls_out = Variable(x.data.clone().resize_(x.size(0), x.size(1), k).fill_(0))\n",
    "#         for i in range(k):\n",
    "#             tmp = tmp + feat[:, :, random.randrange(feat.size(2)), random.randrange(feat.size(3))]\n",
    "#             cls_out[:, :, i] = x[:, :, random.randrange(x.size(2)), random.randrange(x.size(3))]\n",
    "#         return tmp, cls_out\n",
    "    \n",
    "    def forward(self, x1, x2, x3):\n",
    "        return self.forward_single(x1), self.forward_single(x2), self.forward_single(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "net = models.resnet152(pretrained=True)\n",
    "for layer in [net.conv1, net.bn1, net.layer1, net.layer2, net.layer3]:\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = False\n",
    "weird_net = WeirdNet(net, 4).cuda().train()\n",
    "criterion1 = TripletLoss(0.1)\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD((p for p in weird_net.parameters() if p.requires_grad), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def micro_batch(last, i, is_final, batch):\n",
    "    t1_in = torch.Tensor(len(batch), *image_size).cuda()\n",
    "    t2_in = torch.Tensor(len(batch), *image_size).cuda()\n",
    "    t3_in = torch.Tensor(len(batch), *image_size).cuda()\n",
    "    lab1_in = torch.Tensor(len(batch)).cuda()\n",
    "    lab2_in = torch.LongTensor(len(batch)).cuda()\n",
    "    for j, (im, lab) in enumerate(batch):\n",
    "        t1_in[j] = im\n",
    "        im2 = random.choice(dataset)[0]\n",
    "        t2_in[j] = im2\n",
    "        im3 = random.choice(dataset)[0]\n",
    "        t3_in[j] = im3\n",
    "        lab1_in[j] = random.choice([1, -1])\n",
    "        lab2_in[j] = lab\n",
    "    t1_in = Variable(t1_in)\n",
    "    t2_in = Variable(t2_in)\n",
    "    t3_in = Variable(t3_in)\n",
    "    lab1_in = Variable(lab1_in)\n",
    "    lab2_in = Variable(lab2_in)\n",
    "    out = weird_net(t1_in, t2_in, t3_in)\n",
    "    loss = criterion1(out[0][0], out[1][0], out[2][0])\n",
    "    cls_out = out[0][1]\n",
    "    loss2 = criterion2(cls_out[:, :, 0], lab2_in)\n",
    "    k = cls_out.size(2)\n",
    "    for i in range(1, k):\n",
    "        loss2 += criterion2(cls_out[:, :, i], lab2_in)\n",
    "    loss2 /= k\n",
    "    loss += alpha * loss2\n",
    "    loss.backward()\n",
    "    print('Epoch: {0}, Batch: {1}, Micro idx: {2}'.format(ep, last, j))\n",
    "    time.sleep(4)\n",
    "    return last\n",
    "\n",
    "def mini_batch(last, i, is_final, batch):\n",
    "    optimizer.zero_grad()\n",
    "    fold_batches(micro_batch, last, batch, micro_batch_size, cut_end=True)\n",
    "    optimizer.step()\n",
    "    return last + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Micro batch idx: 0\n",
      "Epoch: 0, Micro batch idx: 1\n",
      "Epoch: 0, Micro batch idx: 2\n",
      "Epoch: 0, Micro batch idx: 3\n",
      "Epoch: 0, Micro batch idx: 4\n",
      "Epoch: 0, Micro batch idx: 5\n",
      "Epoch: 0, Micro batch idx: 6\n",
      "Epoch: 0, Micro batch idx: 7\n",
      "Epoch: 0, Micro batch idx: 8\n",
      "Epoch: 0, Micro batch idx: 9\n",
      "Epoch: 0, Micro batch idx: 10\n",
      "Epoch: 0, Micro batch idx: 11\n",
      "Epoch: 0, Micro batch idx: 12\n",
      "Epoch: 0, Micro batch idx: 13\n",
      "Epoch: 0, Micro batch idx: 14\n",
      "Epoch: 0, Micro batch idx: 15\n",
      "Epoch: 0, Micro batch idx: 16\n",
      "Epoch: 0, Micro batch idx: 17\n",
      "Epoch: 0, Micro batch idx: 18\n",
      "Epoch: 0, Micro batch idx: 19\n",
      "Epoch: 0, Micro batch idx: 20\n",
      "Epoch: 0, Micro batch idx: 21\n",
      "Epoch: 0, Micro batch idx: 22\n",
      "Epoch: 0, Micro batch idx: 23\n",
      "Epoch: 0, Micro batch idx: 24\n",
      "Epoch: 0, Micro batch idx: 25\n",
      "Epoch: 0, Micro batch idx: 26\n",
      "Epoch: 0, Micro batch idx: 27\n",
      "Epoch: 0, Micro batch idx: 28\n",
      "Epoch: 0, Micro batch idx: 29\n",
      "Epoch: 0, Micro batch idx: 30\n",
      "Epoch: 0, Micro batch idx: 31\n",
      "Epoch: 0, Micro batch idx: 32\n",
      "Epoch: 0, Micro batch idx: 33\n",
      "Epoch: 0, Micro batch idx: 34\n",
      "Epoch: 0, Micro batch idx: 35\n",
      "Epoch: 0, Micro batch idx: 36\n",
      "Epoch: 0, Micro batch idx: 37\n",
      "Epoch: 0, Micro batch idx: 38\n",
      "Epoch: 0, Micro batch idx: 39\n",
      "Epoch: 0, Micro batch idx: 40\n",
      "Epoch: 0, Micro batch idx: 41\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 1118, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 300, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1044, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1004, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 454, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 500, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python2.7/posixpath.py\", line 382, in realpath\n",
      "    path, ok = _joinrealpath('', filename, {})\n",
      "  File \"/usr/lib/python2.7/posixpath.py\", line 407, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python2.7/posixpath.py\", line 142, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[0;32m   1828\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1830\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1392\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1300\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m             )\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "def create_epoch(epoch, dataset, test_set):\n",
    "    return dataset, {}, {}\n",
    "\n",
    "def create_batch(batch, n):\n",
    "    t1_in = torch.Tensor(len(batch), *image_size).cuda()\n",
    "    t2_in = torch.Tensor(len(batch), *image_size).cuda()\n",
    "    t3_in = torch.Tensor(len(batch), *image_size).cuda()\n",
    "    lab1_in = torch.Tensor(len(batch)).cuda()\n",
    "    lab2_in = torch.LongTensor(len(batch)).cuda()\n",
    "    for j, (im, lab) in enumerate(batch):\n",
    "        t1_in[j] = im\n",
    "        im2 = random.choice(dataset)[0]\n",
    "        t2_in[j] = im2\n",
    "        im3 = random.choice(dataset)[0]\n",
    "        t3_in[j] = im3\n",
    "        lab1_in[j] = random.choice([1, -1])\n",
    "        lab2_in[j] = lab\n",
    "    return [t1_in, t2_in, t3_in], [lab1_in, lab2_in]\n",
    "\n",
    "def create_loss(out, labels_in):\n",
    "    # out is a tuple of 3 tuples, each for the descriptor\n",
    "    # and a tensor with all classification results for the highest\n",
    "    # classification values. the first loss is a simple loss on the\n",
    "    # descriptors. the second loss is a classification loss for\n",
    "    # each sub-region of the input. we simply sum-aggregate here\n",
    "    loss = criterion1(out[0][0], out[1][0], out[2][0])\n",
    "    cls_out = out[0][1]\n",
    "    loss2 = criterion2(cls_out[:, :, 0], labels_in[1])\n",
    "    k = cls_out.size(2)\n",
    "    for i in range(1, k):\n",
    "        loss2 += criterion2(cls_out[:, :, i], labels_in[1])\n",
    "    loss2 /= k\n",
    "    return loss, loss2\n",
    "\n",
    "def micro_batch_gen(last, i, is_final, batch):\n",
    "    prev_loss, mini_batch_size = last\n",
    "    n = len(batch)\n",
    "    tensors_in, labels_in = create_batch(batch, n, **batch_args)\n",
    "    tensors_out = weird_net(*(Variable(t) for t in tensors_in))\n",
    "    loss, loss2 = create_loss(tensors_out, [Variable(l) for l in labels_in])\n",
    "    loss_micro = loss * n / mini_batch_size\n",
    "    val = loss.data[0]\n",
    "    if loss2 is not None:\n",
    "        loss2_micro = loss2 * n / mini_batch_size\n",
    "        val += alpha * (loss2.data[0])\n",
    "        loss_micro += alpha * loss2_micro\n",
    "    loss_micro.backward()\n",
    "    print('Epoch: {0}, Micro batch idx: {1}'.format(epoch, i))\n",
    "    time.sleep(4)\n",
    "    return prev_loss + val, mini_batch_size\n",
    "\n",
    "def mini_batch_gen(last, i, is_final, batch):\n",
    "    batch_count, score, running_loss = last\n",
    "    optimizer.zero_grad()\n",
    "    loss, _ = fold_batches(micro_batch_gen, (0.0, len(batch)), batch, micro_batch_size)\n",
    "    optimizer.step()\n",
    "    return batch_count + 1, score, running_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    dataset, batch_args, stats_args = create_epoch(epoch, dataset, [])\n",
    "\n",
    "    init = 0, 0, 0.0  # batch count, score, running loss\n",
    "    _, best_score, _ = fold_batches(mini_batch_gen, init, dataset, mini_batch_size, cut_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /b/wheel/pytorch-src/torch/lib/THC/generic/THCTensorMath.cu:35",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8b4f8c17767d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfold_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/mrim/kohlm/nnForRetrieval/utils/train_general.pyc\u001b[0m in \u001b[0;36mfold_batches\u001b[1;34m(f, init, x, batch_size, cut_end, add_args)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mis_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcut_end\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0madd_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mrim/kohlm/nnForRetrieval/utils/train_general.pyc\u001b[0m in \u001b[0;36mred\u001b[1;34m(last, idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mis_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcut_end\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0madd_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-f72587801d3d>\u001b[0m in \u001b[0;36mmini_batch\u001b[1;34m(last, i, is_final, batch)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mfold_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmicro_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmicro_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/torch/optim/optimizer.pyc\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /b/wheel/pytorch-src/torch/lib/THC/generic/THCTensorMath.cu:35"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    random.shuffle(dataset)\n",
    "    fold_batches(mini_batch, 0, dataset, mini_batch_size, cut_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
