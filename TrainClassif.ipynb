{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from __future__ import print_function\n",
    "\n",
    "from model import ModelDefinition\n",
    "from dataset import ReadImages, collection\n",
    "import os\n",
    "import os.path as path\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MeanAndStd(imageList, fname=None):\n",
    "    \"\"\"\n",
    "        Take a list of (image name, label) and return the mean and std dev and save it to a file is fname is set\n",
    "    \"\"\"\n",
    "    imageList = ReadImages.openAll(trainset, (225,225))\n",
    "    m = collection.ComputeMean(imageList)\n",
    "    print(\"Mean : \", m)\n",
    "    s = collection.ComputeStdDev(imageList, m)\n",
    "    print(\"std dev : \", s)\n",
    "    if not fname is None:\n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write('%.3f %.3f %.3f' %m)\n",
    "            f.write('\\n')\n",
    "            f.write('%.3f %.3f %.3f' %(s))\n",
    "    return m,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readMeanStd(fname='data/cli.txt'):\n",
    "    with open(fname) as f:\n",
    "        mean = map(float, f.readline().split(' '))\n",
    "        std = map(float, f.readline().split(' '))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNet(net, testset, labels, batchSize=32):\n",
    "    \"\"\"\n",
    "        Test the network accuracy on a testset\n",
    "        Return the number of succes and the number of evaluations done\n",
    "    \"\"\"\n",
    "    net = net.eval() #set the network in eval mode\n",
    "    correct = 0\n",
    "    tot = 0\n",
    "    cpt = 0\n",
    "    for j in range(len(testset)/batchSize):\n",
    "\n",
    "        #set the inputs\n",
    "        inp = torch.Tensor(batchSize,3,225,225).cuda()\n",
    "        for k in range(batchSize):\n",
    "            inp[k] = testTransform(testset[j*batchSize+k][0])\n",
    "            cpt += 1\n",
    "\n",
    "        #forward pass\n",
    "        outputs = net(Variable(inp, volatile=True)) #volatile the free memory after the forward\n",
    "\n",
    "        #compute score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.tolist()\n",
    "        for k in range(batchSize):\n",
    "            if (testset[j*batchSize+k][1] in labels):\n",
    "                correct += (predicted[k][0] == labels.index(testset[j*batchSize+k][1]))\n",
    "                tot += 1\n",
    "\n",
    "    #handle the rest of the testset\n",
    "    rest = len(testset)%batchSize\n",
    "\n",
    "    #set inputs\n",
    "    inp = torch.Tensor(rest,3,225,225).cuda()\n",
    "    for j in range(rest):\n",
    "        inp[j] = testTransform(testset[len(testset)-rest+j][0])\n",
    "\n",
    "    #forward\n",
    "    outputs = mymodel(Variable(inp, volatile=True))\n",
    "\n",
    "    #compute score\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted = predicted.tolist()\n",
    "    for j in range(rest):\n",
    "        if (testset[len(testset)-rest+j][1] in labels):\n",
    "           correct += (predicted[j][0] == labels.index(testset[len(testset)-rest+j][1]))\n",
    "           tot += 1\n",
    "    return correct, tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(mymodel, trainset, testset, imageTransform, testTransform, criterion, optimizer, saveDir=\"data/\", batchSize=32, epochStart=0, nbEpoch=50, bestScore=0):\n",
    "    \"\"\"\n",
    "        Train a network\n",
    "        inputs : \n",
    "            * trainset\n",
    "            * testset, \n",
    "            * transformations to apply to image (for train and for test)\n",
    "            * loss function (criterion)\n",
    "            * optimizer\n",
    "    \"\"\"\n",
    "    for epoch in range(epochStart, nbEpoch): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        random.shuffle(trainset)    \n",
    "        for i in range(len(trainset)/batchSize):\n",
    "            # get the inputs\n",
    "            inputs = torch.Tensor(batchSize,3,225,225).cuda()\n",
    "            for j in range(batchSize):\n",
    "                inputs[j] = imageTransform(trainset[j+i*batchSize][0])\n",
    "            inputs = Variable(inputs)\n",
    "\n",
    "            #get the labels\n",
    "            lab = Variable(torch.LongTensor([labels.index(trainset[j+i*batchSize][1]) for j in range(batchSize)]).cuda())\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = mymodel(inputs)\n",
    "            loss = criterion(outputs, lab)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 10 == 9: # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if i % 50 == 49: #test every 20 mini-batches\n",
    "                print('test :')\n",
    "                c, t = testNet(mymodel, testset, labels, batchSize=batchSize)\n",
    "                print(\"Correct : \", c, \"/\", t)\n",
    "                if (c >= bestScore):\n",
    "                    print(\"Save best model\")\n",
    "                    best = mymodel\n",
    "                    bestScore = c\n",
    "                    torch.save(best, \"bestModel.ckpt\")\n",
    "                #else:\n",
    "                #    mymodel = best\n",
    "                torch.save(mymodel, path.join(saveDir,\"model-\"+str(epoch)+\".ckpt\"))\n",
    "                mymodel.train() #set the model in train mode\n",
    "\n",
    "    print('Finished Training')\n",
    "    return mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copyResNet(net, netBase):\n",
    "    \"\"\"\n",
    "        TODO : make more general\n",
    "    \"\"\"\n",
    "    net.conv1.weight.data = netBase.conv1.weight.data\n",
    "    net.bn1.weight.data = netBase.bn1.weight.data\n",
    "    net.bn1.bias.data = netBase.bn1.bias.data\n",
    "\n",
    "    lLayer = [(net.layer1, netBase.layer1, 3),\n",
    "              (net.layer2, netBase.layer2, 4),\n",
    "              (net.layer3, netBase.layer3, 6),\n",
    "              (net.layer4, netBase.layer4, 3)\n",
    "             ]\n",
    "\n",
    "    for targetLayer, rootLayer, nbC in lLayer:\n",
    "        for i in range(nbC):\n",
    "            targetLayer[i].conv1.weight.data = rootLayer[i].conv1.weight.data\n",
    "            targetLayer[i].bn1.weight.data = rootLayer[i].bn1.weight.data\n",
    "            targetLayer[i].bn1.bias.data = rootLayer[i].bn1.bias.data\n",
    "            targetLayer[i].conv2.weight.data = rootLayer[i].conv2.weight.data\n",
    "            targetLayer[i].bn2.weight.data = rootLayer[i].bn2.weight.data\n",
    "            targetLayer[i].bn2.bias.data = rootLayer[i].bn2.bias.data\n",
    "            targetLayer[i].conv3.weight.data = rootLayer[i].conv3.weight.data\n",
    "            targetLayer[i].bn3.weight.data = rootLayer[i].bn3.weight.data\n",
    "            targetLayer[i].bn3.bias.data = rootLayer[i].bn3.bias.data\n",
    "        targetLayer[0].downsample[0].weight.data = rootLayer[0].downsample[0].weight.data\n",
    "        targetLayer[0].downsample[1].weight.data = rootLayer[0].downsample[1].weight.data\n",
    "        targetLayer[0].downsample[1].bias.data = rootLayer[0].downsample[1].bias.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run1():\n",
    "    \n",
    "    #training and test sets\n",
    "    #trainset = ReadImages.readImageswithPattern('/video/CLICIDE', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    #testset = ReadImages.readImageswithPattern('/video/CLICIDE/test/', lambda x:x.split('/')[-1].split('-')[0])\n",
    "\n",
    "    trainset = ReadImages.readImageswithPattern('/video/fourviere', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    testset = ReadImages.readImageswithPattern('/video/fourviere/test/', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    \n",
    "    \n",
    "    #define the labels list\n",
    "    listLabel = [t[1] for t in trainset if not 'wall' in t[1]]\n",
    "    labels = list(set(listLabel)) #we have to give a number for each label\n",
    "    print(\"There is \", len(labels), \" categories\")\n",
    "    \n",
    "    m, s = readMeanStd('data/fou.txt')\n",
    "    \n",
    "    #print(\"Compute Mean and Std dev on the collection\")\n",
    "    #m, s = MeanAndStd(trainset, \"data/fou.txt\")\n",
    "    \n",
    "    #open the images\n",
    "    #do that only if it fits in memory !\n",
    "    for i in range(len(trainset)):\n",
    "        trainset[i] = (Image.open(trainset[i][0]), trainset[i][1])\n",
    "        \n",
    "    for i in range(len(testset)):\n",
    "        testset[i] = (Image.open(testset[i][0]), testset[i][1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #define the model\n",
    "    mymodel = models.resnet50(len(labels))\n",
    "    copyResNet(mymodel, models.resnet50(pretrained=True))\n",
    "    #ModelDefinition.copyParameters(mymodel, models.alexnet(pretrained=True))\n",
    "    \n",
    "    #or load the model\n",
    "    #mymodel = torch.load('bestModel.ckpt')\n",
    "    \n",
    "    criterion = nn.loss.CrossEntropyLoss()\n",
    "    mymodel.train().cuda()\n",
    "    \n",
    "    #define the optimizer to only the classifier with lr of 1e-2\n",
    "    #optimizer=optim.SGD([\n",
    "    #                {'params': mymodel.classifier.parameters()},\n",
    "    #                {'params': mymodel.features.parameters(), 'lr': 0.0}\n",
    "    #            ], lr=1e-3, momentum=0.9)\n",
    "    optimizer=optim.SGD([\n",
    "                    {'params': mymodel.conv1.parameters(),\n",
    "                     'params': mymodel.bn1.parameters(),\n",
    "                     'params': mymodel.layer1.parameters(),\n",
    "                     'params': mymodel.layer2.parameters(),\n",
    "                     'params': mymodel.layer3.parameters(),\n",
    "                     'params': mymodel.layer4.parameters()},\n",
    "                    {'params': mymodel.fc.parameters(), 'lr': 0.01}\n",
    "                ], lr=0.0, momentum=0.9)\n",
    "\n",
    "    imageTransform = transforms.Compose( (transforms.Scale(300), transforms.RandomCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)) )\n",
    "    testTransform = transforms.Compose( (transforms.Scale(225), transforms.CenterCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)))\n",
    "    batchSize = 64\n",
    "    \n",
    "    train(mymodel, trainset, testset, imageTransform, testTransform, criterion, optimizer, \n",
    "          saveDir=\"data/\", batchSize=32, epochStart=0, nbEpoch=50, bestScore=0)\n",
    "    \n",
    "    #define the optimizer train on all the network\n",
    "    optimizer=optim.SGD(mymodel.parameters(), lr=0.0001, momentum=0.9)\n",
    "    train(mymodel, trainset, testset, imageTransform, testTransform, criterion, optimizer, \n",
    "          saveDir=\"data/\", batchSize=32, epochStart=40, nbEpoch=50, bestScore=290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run2():\n",
    "    trainset = ReadImages.readImageswithPattern('/video/fourviere', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    testset = ReadImages.readImageswithPattern('/video/fourviere/test/', lambda x:x.split('/')[-1].split('-')[0])\n",
    "\n",
    "\n",
    "    #define the labels list\n",
    "    listLabel = [t[1] for t in trainset if not 'wall' in t[1]]\n",
    "    labels = list(set(listLabel)) #we have to give a number for each label\n",
    "    print(\"There is \", len(labels), \" categories\")\n",
    "\n",
    "    m, s = readMeanStd('data/fou.txt')\n",
    "\n",
    "    #print(\"Compute Mean and Std dev on the collection\")\n",
    "    #m, s = MeanAndStd(trainset, \"data/fou.txt\")\n",
    "\n",
    "    #open the images\n",
    "    #do that only if it fits in memory !\n",
    "    for i in range(len(trainset)):\n",
    "        trainset[i] = (Image.open(trainset[i][0]), trainset[i][1])\n",
    "\n",
    "    for i in range(len(testset)):\n",
    "        testset[i] = (Image.open(testset[i][0]), testset[i][1])\n",
    "\n",
    "    criterion = nn.loss.CrossEntropyLoss()\n",
    "    imageTransform = transforms.Compose( (transforms.Scale(300), transforms.RandomCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)) )\n",
    "    testTransform = transforms.Compose( (transforms.Scale(225), transforms.CenterCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)))\n",
    "    mymodel = torch.load('bestModel.ckpt')\n",
    "    mymodel.train().cuda()\n",
    "    optimizer=optim.SGD(mymodel.parameters(), lr=0.0001, momentum=0.9)\n",
    "    train(mymodel, trainset, testset, imageTransform, testTransform, criterion, optimizer, \n",
    "          saveDir=\"data/\", batchSize=32, epochStart=37, nbEpoch=50, bestScore=308)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
