{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from __future__ import print_function\n",
    "\n",
    "from model import ModelDefinition\n",
    "from dataset import ReadImages, collection\n",
    "import os\n",
    "import os.path as path\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MeanAndStd(imageList, fname=None):\n",
    "    \"\"\"\n",
    "        Take a list of (image name, label) and return the mean and std dev and save it to a file is fname is set\n",
    "    \"\"\"\n",
    "    imageList = ReadImages.openAll(trainset, (225,225))\n",
    "    m = collection.ComputeMean(imageList)\n",
    "    print(\"Mean : \", m)\n",
    "    s = collection.ComputeStdDev(imageList, m)\n",
    "    print(\"std dev : \", s)\n",
    "    if not fname is None:\n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write('%.3f %.3f %.3f' %m)\n",
    "            f.write('\\n')\n",
    "            f.write('%.3f %.3f %.3f' %(s))\n",
    "    return m,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readMeanStd(fname='data/cli.txt'):\n",
    "    with open(fname) as f:\n",
    "        mean = map(float, f.readline().split(' '))\n",
    "        std = map(float, f.readline().split(' '))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNet(net, testset, labels, batchSize=32):\n",
    "    \"\"\"\n",
    "        Test the network accuracy on a testset\n",
    "        Return the number of succes and the number of evaluations done\n",
    "    \"\"\"\n",
    "    net = net.eval() #set the network in eval mode\n",
    "    correct = 0\n",
    "    tot = 0\n",
    "    cpt = 0\n",
    "    for j in range(len(testset)/batchSize):\n",
    "\n",
    "        #set the inputs\n",
    "        inp = torch.Tensor(batchSize,3,225,225).cuda()\n",
    "        for k in range(batchSize):\n",
    "            inp[k] = testTransform(testset[j*batchSize+k][0])\n",
    "            cpt += 1\n",
    "\n",
    "        #forward pass\n",
    "        outputs = net(Variable(inp, volatile=True)) #volatile the free memory after the forward\n",
    "\n",
    "        #compute score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.tolist()\n",
    "        for k in range(batchSize):\n",
    "            if (testset[j*batchSize+k][1] in labels):\n",
    "                correct += (predicted[k][0] == labels.index(testset[j*batchSize+k][1]))\n",
    "                tot += 1\n",
    "\n",
    "    #handle the rest of the testset\n",
    "    rest = len(testset)%batchSize\n",
    "\n",
    "    #set inputs\n",
    "    inp = torch.Tensor(rest,3,225,225).cuda()\n",
    "    for j in range(rest):\n",
    "        inp[j] = testTransform(testset[len(testset)-rest+j][0])\n",
    "\n",
    "    #forward\n",
    "    outputs = mymodel(Variable(inp, volatile=True))\n",
    "\n",
    "    #compute score\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted = predicted.tolist()\n",
    "    for j in range(rest):\n",
    "        if (testset[len(testset)-rest+j][1] in labels):\n",
    "           correct += (predicted[j][0] == labels.index(testset[len(testset)-rest+j][1]))\n",
    "           tot += 1\n",
    "    return correct, tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(mymodel, trainset, testset, labels, imageTransform, testTransform, criterion, optimizer, saveDir=\"data/\", batchSize=32, epochStart=0, nbEpoch=50, bestScore=0):\n",
    "    \"\"\"\n",
    "        Train a network\n",
    "        inputs : \n",
    "            * trainset\n",
    "            * testset, \n",
    "            * transformations to apply to image (for train and for test)\n",
    "            * loss function (criterion)\n",
    "            * optimizer\n",
    "    \"\"\"\n",
    "    for epoch in range(epochStart, nbEpoch): # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        random.shuffle(trainset)    \n",
    "        for i in range(len(trainset)/batchSize):\n",
    "            # get the inputs\n",
    "            inputs = torch.Tensor(batchSize,3,225,225).cuda()\n",
    "            for j in range(batchSize):\n",
    "                inputs[j] = imageTransform(trainset[j+i*batchSize][0])\n",
    "            inputs = Variable(inputs)\n",
    "\n",
    "            #get the labels\n",
    "            lab = Variable(torch.LongTensor([labels.index(trainset[j+i*batchSize][1]) for j in range(batchSize)]).cuda())\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = mymodel(inputs)\n",
    "            loss = criterion(outputs, lab)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 10 == 9: # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if i % 50 == 49: #test every 20 mini-batches\n",
    "                print('test :')\n",
    "                c, t = testNet(mymodel, testset, labels, batchSize=batchSize)\n",
    "                print(\"Correct : \", c, \"/\", t)\n",
    "                if (c >= bestScore):\n",
    "                    print(\"Save best model\")\n",
    "                    best = mymodel\n",
    "                    bestScore = c\n",
    "                    torch.save(best, \"bestModel.ckpt\")\n",
    "                #else:\n",
    "                #    mymodel = best\n",
    "                torch.save(mymodel, path.join(saveDir,\"model-\"+str(epoch)+\".ckpt\"))\n",
    "                mymodel.train() #set the model in train mode\n",
    "\n",
    "    print('Finished Training')\n",
    "    return bestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copyResNet(net, netBase):\n",
    "    \"\"\"\n",
    "        TODO : make more general\n",
    "    \"\"\"\n",
    "    net.conv1.weight.data = netBase.conv1.weight.data\n",
    "    net.bn1.weight.data = netBase.bn1.weight.data\n",
    "    net.bn1.bias.data = netBase.bn1.bias.data\n",
    "\n",
    "    lLayer = [(net.layer1, netBase.layer1, 3),\n",
    "              (net.layer2, netBase.layer2, 4),\n",
    "              (net.layer3, netBase.layer3, 6),\n",
    "              (net.layer4, netBase.layer4, 3)\n",
    "             ]\n",
    "\n",
    "    for targetLayer, rootLayer, nbC in lLayer:\n",
    "        for i in range(nbC):\n",
    "            targetLayer[i].conv1.weight.data = rootLayer[i].conv1.weight.data\n",
    "            targetLayer[i].bn1.weight.data = rootLayer[i].bn1.weight.data\n",
    "            targetLayer[i].bn1.bias.data = rootLayer[i].bn1.bias.data\n",
    "            targetLayer[i].conv2.weight.data = rootLayer[i].conv2.weight.data\n",
    "            targetLayer[i].bn2.weight.data = rootLayer[i].bn2.weight.data\n",
    "            targetLayer[i].bn2.bias.data = rootLayer[i].bn2.bias.data\n",
    "            targetLayer[i].conv3.weight.data = rootLayer[i].conv3.weight.data\n",
    "            targetLayer[i].bn3.weight.data = rootLayer[i].bn3.weight.data\n",
    "            targetLayer[i].bn3.bias.data = rootLayer[i].bn3.bias.data\n",
    "        targetLayer[0].downsample[0].weight.data = rootLayer[0].downsample[0].weight.data\n",
    "        targetLayer[0].downsample[1].weight.data = rootLayer[0].downsample[1].weight.data\n",
    "        targetLayer[0].downsample[1].bias.data = rootLayer[0].downsample[1].bias.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run1():\n",
    "    \n",
    "    #training and test sets\n",
    "    #trainset = ReadImages.readImageswithPattern('/video/CLICIDE', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    #testset = ReadImages.readImageswithPattern('/video/CLICIDE/test/', lambda x:x.split('/')[-1].split('-')[0])\n",
    "\n",
    "    trainset = ReadImages.readImageswithPattern('/video/fourviere', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    testset = ReadImages.readImageswithPattern('/video/fourviere/test/', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    \n",
    "    \n",
    "    #define the labels list\n",
    "    listLabel = [t[1] for t in trainset if not 'wall' in t[1]] + [t[1] for t in testset if not 'wall' in t[1]]\n",
    "    labels = list(set(listLabel)) #we have to give a number for each label\n",
    "    print(\"There is \", len(labels), \" categories\")\n",
    "    \n",
    "    m, s = readMeanStd('data/fou.txt')\n",
    "    \n",
    "    #print(\"Compute Mean and Std dev on the collection\")\n",
    "    #m, s = MeanAndStd(trainset, \"data/fou.txt\")\n",
    "    \n",
    "    #open the images\n",
    "    #do that only if it fits in memory !\n",
    "    for i in range(len(trainset)):\n",
    "        trainset[i] = (Image.open(trainset[i][0]), trainset[i][1])\n",
    "        \n",
    "    for i in range(len(testset)):\n",
    "        testset[i] = (Image.open(testset[i][0]), testset[i][1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #define the model\n",
    "    mymodel = models.resnet50(len(labels))\n",
    "    copyResNet(mymodel, models.resnet50(pretrained=True))\n",
    "    #ModelDefinition.copyParameters(mymodel, models.alexnet(pretrained=True))\n",
    "    \n",
    "    #or load the model\n",
    "    #mymodel = torch.load('bestModel.ckpt')\n",
    "    \n",
    "    criterion = nn.loss.CrossEntropyLoss()\n",
    "    mymodel.train().cuda()\n",
    "    \n",
    "    #define the optimizer to only the classifier with lr of 1e-2\n",
    "    #optimizer=optim.SGD([\n",
    "    #                {'params': mymodel.classifier.parameters()},\n",
    "    #                {'params': mymodel.features.parameters(), 'lr': 0.0}\n",
    "    #            ], lr=1e-3, momentum=0.9)\n",
    "    optimizer=optim.SGD([\n",
    "                    {'params': mymodel.conv1.parameters(),\n",
    "                     'params': mymodel.bn1.parameters(),\n",
    "                     'params': mymodel.layer1.parameters(),\n",
    "                     'params': mymodel.layer2.parameters(),\n",
    "                     'params': mymodel.layer3.parameters(),\n",
    "                     'params': mymodel.layer4.parameters()},\n",
    "                    {'params': mymodel.fc.parameters(), 'lr': 0.01}\n",
    "                ], lr=0.0, momentum=0.9)\n",
    "\n",
    "    imageTransform = transforms.Compose( (transforms.Scale(300), transforms.RandomCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)) )\n",
    "    testTransform = transforms.Compose( (transforms.Scale(225), transforms.CenterCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)))\n",
    "    batchSize = 64\n",
    "    \n",
    "    train(mymodel, trainset, testset, labels, imageTransform, testTransform, criterion, optimizer, \n",
    "          saveDir=\"data/\", batchSize=32, epochStart=0, nbEpoch=50, bestScore=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run2():\n",
    "    trainset = ReadImages.readImageswithPattern('/video/fourviere', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    testset = ReadImages.readImageswithPattern('/video/fourviere/test/', lambda x:x.split('/')[-1].split('-')[0])\n",
    "\n",
    "\n",
    "    #define the labels list\n",
    "    listLabel = [t[1] for t in trainset if not 'wall' in t[1]]\n",
    "    labels = list(set(listLabel)) #we have to give a number for each label\n",
    "    print(\"There is \", len(labels), \" categories\")\n",
    "\n",
    "    m, s = readMeanStd('data/fou.txt')\n",
    "\n",
    "    #print(\"Compute Mean and Std dev on the collection\")\n",
    "    #m, s = MeanAndStd(trainset, \"data/fou.txt\")\n",
    "\n",
    "    #open the images\n",
    "    #do that only if it fits in memory !\n",
    "    for i in range(len(trainset)):\n",
    "        trainset[i] = (Image.open(trainset[i][0]), trainset[i][1])\n",
    "\n",
    "    for i in range(len(testset)):\n",
    "        testset[i] = (Image.open(testset[i][0]), testset[i][1])\n",
    "\n",
    "    criterion = nn.loss.CrossEntropyLoss()\n",
    "    imageTransform = transforms.Compose( (transforms.Scale(300), transforms.RandomCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)) )\n",
    "    testTransform = transforms.Compose( (transforms.Scale(225), transforms.CenterCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)))\n",
    "    mymodel = torch.load('bestModel.ckpt')\n",
    "    mymodel.train().cuda()\n",
    "    optimizer=optim.SGD(mymodel.parameters(), lr=0.0001, momentum=0.9)\n",
    "    train(mymodel, trainset, testset, labels, imageTransform, testTransform, criterion, optimizer, \n",
    "          saveDir=\"data/\", batchSize=32, epochStart=37, nbEpoch=50, bestScore=308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run3():\n",
    "    #training and test sets\n",
    "    trainset = ReadImages.readImageswithPattern('/video/CLICIDE', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    testset = ReadImages.readImageswithPattern('/video/CLICIDE/test/', lambda x:x.split('/')[-1].split('-')[0])\n",
    "    \n",
    "    \n",
    "    #define the labels list\n",
    "    listLabel = [t[1] for t in trainset if not 'wall' in t[1]]\n",
    "    labels = list(set(listLabel)) #we have to give a number for each label\n",
    "    print(\"There is \", len(labels), \" categories\")\n",
    "    \n",
    "    m, s = readMeanStd('data/cli.txt')\n",
    "    \n",
    "    #print(\"Compute Mean and Std dev on the collection\")\n",
    "    #m, s = MeanAndStd(trainset, \"data/fou.txt\")\n",
    "    \n",
    "    #open the images\n",
    "    #do that only if it fits in memory !\n",
    "    for i in range(len(trainset)):\n",
    "        trainset[i] = (Image.open(trainset[i][0]), trainset[i][1])\n",
    "        \n",
    "    for i in range(len(testset)):\n",
    "        testset[i] = (Image.open(testset[i][0]), testset[i][1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #define the model\n",
    "    #mymodel = models.resnet50(len(labels))\n",
    "    #copyResNet(mymodel, models.resnet50(pretrained=True))\n",
    "    #ModelDefinition.copyParameters(mymodel, models.alexnet(pretrained=True))\n",
    "    \n",
    "    #or load the model\n",
    "    mymodel = torch.load('bestModel.ckpt')\n",
    "    \n",
    "    criterion = nn.loss.CrossEntropyLoss()\n",
    "    mymodel.train().cuda()\n",
    "    \n",
    "    optimizer=optim.SGD([\n",
    "                    {'params': mymodel.conv1.parameters(),\n",
    "                     'params': mymodel.bn1.parameters(),\n",
    "                     'params': mymodel.layer1.parameters(),\n",
    "                     'params': mymodel.layer2.parameters(),\n",
    "                     'params': mymodel.layer3.parameters(),\n",
    "                     'params': mymodel.layer4.parameters()},\n",
    "                    {'params': mymodel.fc.parameters(), 'lr': 0.01}\n",
    "                ], lr=0.0, momentum=0.9)\n",
    "\n",
    "    imageTransform = transforms.Compose( (transforms.Scale(300), transforms.RandomCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)) )\n",
    "    testTransform = transforms.Compose( (transforms.Scale(225), transforms.CenterCrop(225), transforms.ToTensor(), transforms.Normalize(m,s)))\n",
    "    batchSize = 64\n",
    "    \n",
    "    #best = train(mymodel, trainset, testset, labels, imageTransform, testTransform, criterion, optimizer, \n",
    "    #      saveDir=\"data/\", batchSize=batchSize, epochStart=0, nbEpoch=40, bestScore=0)\n",
    "    \n",
    "    #print(\"Best score after 40 epoch : \", best)\n",
    "    \n",
    "    mymodel = torch.load('bestModel.ckpt')\n",
    "    mymodel.train().cuda()\n",
    "    optimizer=optim.SGD([\n",
    "                    {'params': mymodel.conv1.parameters(),\n",
    "                     'params': mymodel.bn1.parameters(),\n",
    "                     'params': mymodel.layer1.parameters(),\n",
    "                     'params': mymodel.layer2.parameters(),\n",
    "                     'params': mymodel.layer3.parameters(),\n",
    "                     'params': mymodel.layer4.parameters()},\n",
    "                    {'params': mymodel.fc.parameters(), 'lr': 0.001}\n",
    "                ], lr=0.0, momentum=0.9)\n",
    "\n",
    "    best = train(mymodel, trainset, testset, labels, imageTransform, testTransform, criterion, optimizer, \n",
    "          saveDir=\"data/\", batchSize=batchSize, epochStart=40, nbEpoch=50, bestScore=0)\n",
    "    print(\"Best score after finetuning : \", best)\n",
    "    \n",
    "\n",
    "    #define the optimizer train on all the network\n",
    "    mymodel = torch.load('bestModel.ckpt')\n",
    "    mymodel.train().cuda()\n",
    "    optimizer=optim.SGD(mymodel.parameters(), lr=0.0001, momentum=0.9)\n",
    "    best = train(mymodel, trainset, testset, labels, imageTransform, testTransform, criterion, optimizer, \n",
    "          saveDir=\"data/\", batchSize=batchSize, epochStart=40, nbEpoch=50, bestScore=best)\n",
    "    print(\"Best score after full finetuning : \", best)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is  464  categories\n",
      "[41,    10] loss: 0.239\n",
      "[41,    20] loss: 0.191\n",
      "[41,    30] loss: 0.243\n",
      "[41,    40] loss: 0.238\n",
      "[41,    50] loss: 0.184\n",
      "test :\n",
      "Correct :  62 / 165\n",
      "Save best model\n",
      "[42,    10] loss: 0.223\n",
      "[42,    20] loss: 0.240\n",
      "[42,    30] loss: 0.194\n",
      "[42,    40] loss: 0.220\n",
      "[42,    50] loss: 0.232\n",
      "test :\n",
      "Correct :  61 / 165\n",
      "[43,    10] loss: 0.202\n",
      "[43,    20] loss: 0.205\n",
      "[43,    30] loss: 0.244\n",
      "[43,    40] loss: 0.200\n",
      "[43,    50] loss: 0.189\n",
      "test :\n",
      "Correct :  62 / 165\n",
      "Save best model\n",
      "[44,    10] loss: 0.242\n",
      "[44,    20] loss: 0.191\n",
      "[44,    30] loss: 0.177\n",
      "[44,    40] loss: 0.209\n",
      "[44,    50] loss: 0.219\n",
      "test :\n",
      "Correct :  63 / 165\n",
      "Save best model\n",
      "[45,    10] loss: 0.244\n",
      "[45,    20] loss: 0.220\n",
      "[45,    30] loss: 0.166\n",
      "[45,    40] loss: 0.229\n",
      "[45,    50] loss: 0.215\n",
      "test :\n",
      "Correct :  61 / 165\n",
      "[46,    10] loss: 0.183\n",
      "[46,    20] loss: 0.227\n",
      "[46,    30] loss: 0.216\n",
      "[46,    40] loss: 0.219\n",
      "[46,    50] loss: 0.210\n",
      "test :\n",
      "Correct :  59 / 165\n",
      "[47,    10] loss: 0.241\n",
      "[47,    20] loss: 0.217\n",
      "[47,    30] loss: 0.168\n",
      "[47,    40] loss: 0.194\n",
      "[47,    50] loss: 0.218\n",
      "test :\n",
      "Correct :  63 / 165\n",
      "Save best model\n",
      "[48,    10] loss: 0.235\n",
      "[48,    20] loss: 0.198\n",
      "[48,    30] loss: 0.222\n",
      "[48,    40] loss: 0.202\n",
      "[48,    50] loss: 0.216\n",
      "test :\n",
      "Correct :  61 / 165\n",
      "[49,    10] loss: 0.180\n",
      "[49,    20] loss: 0.264\n",
      "[49,    30] loss: 0.196\n",
      "[49,    40] loss: 0.190\n",
      "[49,    50] loss: 0.226\n",
      "test :\n",
      "Correct :  60 / 165\n",
      "[50,    10] loss: 0.218\n",
      "[50,    20] loss: 0.209\n",
      "[50,    30] loss: 0.180\n",
      "[50,    40] loss: 0.225\n",
      "[50,    50] loss: 0.207\n",
      "test :\n",
      "Correct :  63 / 165\n",
      "Save best model\n",
      "Finished Training\n",
      "Best score after finetuning :  63\n",
      "[41,    10] loss: 0.193\n",
      "[41,    20] loss: 0.202\n",
      "[41,    30] loss: 0.205\n",
      "[41,    40] loss: 0.189\n",
      "[41,    50] loss: 0.221\n",
      "test :\n",
      "Correct :  62 / 165\n",
      "[42,    10] loss: 0.194\n",
      "[42,    20] loss: 0.187\n",
      "[42,    30] loss: 0.197\n",
      "[42,    40] loss: 0.208\n",
      "[42,    50] loss: 0.207\n",
      "test :\n",
      "Correct :  61 / 165\n",
      "[43,    10] loss: 0.192\n",
      "[43,    20] loss: 0.187\n",
      "[43,    30] loss: 0.199\n",
      "[43,    40] loss: 0.204\n",
      "[43,    50] loss: 0.196\n",
      "test :\n",
      "Correct :  62 / 165\n",
      "[44,    10] loss: 0.159\n",
      "[44,    20] loss: 0.165\n",
      "[44,    30] loss: 0.158\n",
      "[44,    40] loss: 0.196\n",
      "[44,    50] loss: 0.228\n",
      "test :\n",
      "Correct :  61 / 165\n",
      "[45,    10] loss: 0.172\n",
      "[45,    20] loss: 0.208\n",
      "[45,    30] loss: 0.166\n",
      "[45,    40] loss: 0.205\n",
      "[45,    50] loss: 0.182\n",
      "test :\n",
      "Correct :  61 / 165\n",
      "[46,    10] loss: 0.157\n",
      "[46,    20] loss: 0.184\n",
      "[46,    30] loss: 0.193\n",
      "[46,    40] loss: 0.149\n",
      "[46,    50] loss: 0.163\n",
      "test :\n",
      "Correct :  63 / 165\n",
      "Save best model\n",
      "[47,    10] loss: 0.161\n",
      "[47,    20] loss: 0.165\n",
      "[47,    30] loss: 0.165\n",
      "[47,    40] loss: 0.187\n",
      "[47,    50] loss: 0.175\n",
      "test :\n",
      "Correct :  62 / 165\n",
      "[48,    10] loss: 0.146\n",
      "[48,    20] loss: 0.128\n",
      "[48,    30] loss: 0.169\n",
      "[48,    40] loss: 0.167\n",
      "[48,    50] loss: 0.165\n",
      "test :\n",
      "Correct :  63 / 165\n",
      "Save best model\n",
      "[49,    10] loss: 0.160\n",
      "[49,    20] loss: 0.170\n",
      "[49,    30] loss: 0.195\n",
      "[49,    40] loss: 0.126\n",
      "[49,    50] loss: 0.161\n",
      "test :\n",
      "Correct :  60 / 165\n",
      "[50,    10] loss: 0.139\n",
      "[50,    20] loss: 0.125\n",
      "[50,    30] loss: 0.165\n",
      "[50,    40] loss: 0.139\n",
      "[50,    50] loss: 0.157\n",
      "test :\n",
      "Correct :  63 / 165\n",
      "Save best model\n",
      "Finished Training\n",
      "Best score after full finetuning :  63\n"
     ]
    }
   ],
   "source": [
    "run3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SGD in module torch.optim.sgd object:\n",
      "\n",
      "class SGD(torch.optim.optimizer.Optimizer)\n",
      " |  Implements stochastic gradient descent (optionally with momentum).\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float): learning rate\n",
      " |      momentum (float, optional): momentum factor (default: 0)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      " |      >>> optimizer.zero_grad()\n",
      " |      >>> loss_fn(model(input), target).backward()\n",
      " |      >>> optimizer.step()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=<object object>, momentum=0, dampening=0, weight_decay=0)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          closure (callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * state - a dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes.\n",
      " |      * param_groups - a dict containig all parameter groups\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Clears the gradients of all optimized :class:`Variable` s.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
